<!DOCTYPE html><html lang="en-US" mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="深度学习中的各种卷积" /><meta name="author" content="darkknightzh" /><meta property="og:locale" content="en_US" /><meta name="description" content="仅供学习交流使用，错误之处在所难免，欢迎指正." /><meta property="og:description" content="仅供学习交流使用，错误之处在所难免，欢迎指正." /><link rel="canonical" href="https://darkknightzh.github.io/posts/differentconv/" /><meta property="og:url" content="https://darkknightzh.github.io/posts/differentconv/" /><meta property="og:site_name" content="darkknightzh" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-07-23T16:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="深度学习中的各种卷积" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"darkknightzh"},"dateModified":"2021-10-12T22:37:38+08:00","datePublished":"2021-07-23T16:00:00+08:00","description":"仅供学习交流使用，错误之处在所难免，欢迎指正.","headline":"深度学习中的各种卷积","mainEntityOfPage":{"@type":"WebPage","@id":"https://darkknightzh.github.io/posts/differentconv/"},"url":"https://darkknightzh.github.io/posts/differentconv/"}</script><title>深度学习中的各种卷积 | darkknightzh</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] }, // chtml: { displayAlign: 'left' } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/avatar.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">darkknightzh</a></div><div class="site-subtitle font-italic">忘记一个人，从忘记那个声音开始</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>主页</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>标签</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>时间轴</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>关于</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/darkknightzh" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['',''].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>深度学习中的各种卷积</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>深度学习中的各种卷积</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> darkknightzh </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Fri, Jul 23, 2021, 4:00 PM +0800" prep="on" > Jul 23, 2021 <i class="unloaded">2021-07-23T16:00:00+08:00</i> </span></div><div> <span> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Tue, Oct 12, 2021, 10:37 PM +0800" prefix="Updated " > Oct 12, 2021 <i class="unloaded">2021-10-12T22:37:38+08:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="6458 words">35 min</span></div></div><div class="post-content"><style> h1 { border-bottom: none }</style><p>转载请注明出处：</p><p><a href="https://darkknightzh.github.io/posts/differentconv">https://darkknightzh.github.io/posts/differentconv</a></p><p>参考网址：</p><p><a href="https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215">https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215</a></p><p><a href="https://zhuanlan.zhihu.com/p/257145620">https://zhuanlan.zhihu.com/p/257145620</a></p><p>说明：大部分都是翻译自第一个参考网址</p><p><br /></p><h1 id="p1-卷积">P1. 卷积</h1><p>深度学习中2D卷积如下图所示，Dout个Din*h*w的卷积核分别和输入特征进行卷积，得到Dout个Hout*Wout的特征（每个特征都只有1个通道），再将这些特征特征拼接，得到Dout*H*W的输出（如下图a）。由于每个Din*h*w的卷积核只在输入特征的H和W方向上滑动（如下图b），因而称作2D卷积。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/1_1a.png" alt="1_1a" /> <em>（a）</em></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/1_1b.png" alt="1_1b" /> <em>（b）</em></p><p>不考虑batch时，输入特征input尺寸为 \(\left[ { {C}_{in}},{ {H}_{in}},{ {W}_{in}} \right]\) ，卷积核weight尺寸 \(\left[ { {C}_{out}},{ {C}_{in}},{ {H}_{k}},{ {W}_{k}} \right]\) ，偏置bias尺寸 \(\left[ { {C}_{out}} \right]\) ，输出特征out尺寸为 \(\left[ { {C}_{out}},{ {H}_{out}},{ {W}_{out}} \right]\) ， 2D卷积参数量在不考虑bias时为 \({ {C}_{out}}\times { {C}_{in}}\times { {H}_{k}}\times { {W}_{k}}\) ；考虑bias时为 \({ {C}_{out}}\times { {C}_{in}}\times { {H}_{k}}\times { {W}_{k}}+{ {C}_{out}}\) 。</p><p>2D卷积的计算量： \({ {C}_{out}}\times { {C}_{in}}\times { {H}_{k}}\times { {W}_{k}}\times \left( { {H}_{in}}-{ {H}_{k}}+1 \right)\times \left( { {W}_{in}}-{ {W}_{k}}+1 \right)\)</p><p>2D卷积输出特征大小为：</p>\[{ {H}_{out}}=\left\lfloor \frac{ { {H}_{in}}+2\times padding[0]-dilation[0]\times (kernel\_size[0]-1)-1}{stride[0]}+1 \right\rfloor\] \[{ {W}_{out}}=\left\lfloor \frac{ { {W}_{in}}+2\times padding[1]-dilation[1]\times (kernel\_size[1]-1)-1}{stride[1]}+1 \right\rfloor\]<p>2D卷积能在2维空间中编码目标的空间关系。</p><h1 id="p2-3d卷积">P2. 3D卷积</h1><p>2D卷积是指在3D立方体数据上进行的2D卷积，滤波器通道数Cin和输入特征通道数Cin相同，每个卷积核只在输入特征的高、宽这两个方向上滑动。在3D卷积中，滤波器的通道数小于输入特征的通道数，因而每个卷积核可以在输入特征的通道、高、宽这3个方向上滑动，因而输出为3D数据。如下图。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/2.png" alt="2" /></p><p>假定3D卷积输入特征 \(\left[ { {C}_{in}},{ {D}_{in}},{ {H}_{in}},{ {W}_{in}} \right]\) ，卷积核尺寸 \(\left[ { {C}_{out}},{ {C}_{in}},{ {D}_{k}},{ {H}_{k}},{ {W}_{k}} \right]\) ，在具体计算时， \({ {C}_{out}}\) 个 \(\left[ { {C}_{in}},{ {D}_{k}},{ {H}_{k}},{ {W}_{k}} \right]\) 的卷积核和输入特征进行卷积，得到 \({ {C}_{out}}\) 个 \(\left[ { {D}_{out}},{ {H}_{out}},{ {W}_{out}} \right]\) 的特征，拼接后得到 \(\left[ { {C}_{out}},{ {D}_{out}},{ {H}_{out}},{ {W}_{out}} \right]\) 的输出。</p><p>3D卷积参数量在不考虑bias时为 \({ {C}_{out}}\times { {C}_{in}}\times { {D}_{k}}\times { {H}_{k}}\times { {W}_{k}}\) ；考虑bias时为 \({ {C}_{out}}\times { {C}_{in}}\times { {D}_{k}}\times { {H}_{k}}\times { {W}_{k}}+{ {C}_{out}}\) 。</p><p>具体可参见下面代码：</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">from</span> <span class="n">__future__</span> <span class="kn">import</span> <span class="n">division</span>

<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">testNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
   <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
      <span class="nf">super</span><span class="p">(</span><span class="n">testNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
      <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv3d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

   <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">get_total_params</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
   <span class="n">model_parameters</span> <span class="o">=</span> <span class="nf">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
   <span class="n">num_params</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">size</span><span class="p">())</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model_parameters</span><span class="p">])</span>
   <span class="k">return</span> <span class="n">num_params</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
   <span class="n">net</span> <span class="o">=</span> <span class="nf">testNet</span><span class="p">()</span>
   <span class="nf">print</span><span class="p">(</span><span class="nf">get_total_params</span><span class="p">(</span><span class="n">net</span><span class="p">))</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
   <span class="nf">main</span><span class="p">()</span>
</pre></table></code></div></div><p>上面代码3D卷积无bias，此时输出结果为3600</p><p>3600=10*3*6*5*4</p><p>有bias时输出结果为3610=10*3*6*5*4+10</p><p>3D卷积输出特征大小为：</p>\[{ {D}_{out}}=\left\lfloor \frac{ { {D}_{in}}+2\times padding[0]-dilation[0]\times (kernel\_size[0]-1)-1}{stride[0]}+1 \right\rfloor\] \[{ {H}_{out}}=\left\lfloor \frac{ { {H}_{in}}+2\times padding[1]-dilation[1]\times (kernel\_size[1]-1)-1}{stride[1]}+1 \right\rfloor\] \[{ {W}_{out}}=\left\lfloor \frac{ { {W}_{in}}+2\times padding[2]-dilation[2]\times (kernel\_size[2]-1)-1}{stride[2]}+1 \right\rfloor\]<p>3D卷积能在3D空间中编码空间关系。这种3D关系在某些应用中很重要，如生物医学成像中的3D分割/重建、CT和MRI等在3D空间中弯曲的血管。</p><h1 id="p3-转置卷积反卷积transposed-convolution-deconvolution">P3. 转置卷积（反卷积，Transposed Convolution, Deconvolution）</h1><h2 id="p31-转置卷积">P3.1. 转置卷积</h2><p>反卷积为卷积的逆过程，后来不常用反卷积，而常使用转置卷积，用来对输入特征进行上采样，提高输出特征分辨率，增大输出特征感受野。</p><p>可以使用卷积实现转置卷积。下图使用3*3的核，并使用2*2的padding（填充0），将2*2的输入上采样成4*4的输出。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/3_1.gif" alt="3_1" /></p><p>将输入特征之间插入不同0之后，可以讲输入映射到不同的输出尺寸，如下图，将2*2的输入，每个特征之间插入1个0，映射到5*5的输出（padding仍旧为2*2）。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/3_2.gif" alt="3_2" /></p><p>令C代表卷积的卷积核，large代表输入图像，small代表卷积后的输出图像。通过卷积（矩阵乘法），将大图像large下采样到小图像small：C*large=small。如下图，将输入平铺为16*1的矩阵，并将间距和变换为4*16的稀疏矩阵。之后应用矩阵乘法，得到4*1的矩阵（4*16 * 16*1=4*1），并变换回2*2的输出。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/3_3.png" alt="3_3" /></p><p>如果在C*large=small两边都乘上 \({ {C}^{T}}\) ，可得到 \({ {C}^{T}}*small=large\) （只有在卷积核C为正交矩阵时， \({ {C}^{T}}*C=I\) 才成立），如下图所示。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/3_4.png" alt="3_4" /></p><p>这就是“转置卷积”的由来。转置卷积的输出分辨率见<a href="https://arxiv.org/abs/1603.07285">https://arxiv.org/abs/1603.07285</a>中Relationship 13 and Relationship 14。</p><h2 id="p32-棋盘格伪影">P3.2. 棋盘格伪影</h2><p>使用转置卷积时会出现棋盘格伪影（Checkerboard artifacts），如下图所示。具体可参见论文“Deconvolution and Checkerboard Artifacts”。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/3_5.png" alt="3_5" /></p><p>主要原因如下：</p><p>棋盘格伪影由转置卷积的不均匀重叠（uneven overlap）引发。这种重叠导致输出特征从输入特征获取到的信息的数量不同（Such overlap puts more of the metaphorical paint in some places than others.）。</p><p>下图的顶端为输入，底端为转置卷积的输出。经过转置卷积，小尺寸的输入映射到大尺寸的输出。（a）的stride=1，滤波器大小为2，如红色区域，输入的第一个像素映射到输出的第一个和第二个像素。输入的第一个和第三个像素均会映射到输出的第二个像素。总体上来说，输出的中间部分的像素从输入像素获取相同数量的信息。这里存在卷积核重叠的区域。当卷积核尺寸增大到图（b）中的3时，接收相同数量信息的中心部分会收缩（范围变小）。由于重叠是均匀的，因而这不是大问题。输出特征的中心部分仍旧会从输入特征获取相同数量的信息。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/3_6.png" alt="3_6" /></p><p>当stride=2时，下图（a）中滤波器大小=2，输出特征的所有像素从输入获取相同数量的信息（均从输入特征的单个像素获取信息），转置卷积在这种情况下没有重叠现象。当设置滤波器大小=4时，如图（b）所示，均匀重叠区域缩小，但是仍旧能使用输出特征的中心区域作为有效输出，这部分区域内每个像素从输入获取相同数量的信息。当设置滤波器大小为3或5，如图（c）和（d），此时输出特征的每个像素相比其相邻像素会从输入获取不同数量的信息。此时输出没有连续且均匀重叠的区域。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/3_7.png" alt="3_7" /></p><p>当滤波器大小不能被stride整除时，转置卷积会有不均匀重叠。不均匀重叠导致输出特征从输入特征获取的信息数量不同，产生了棋盘格伪影。实际上，在二维上两个方向的不均匀相乘，不均匀变成了平方关系，因而不均匀重叠在二维上更加严重。</p><p>使用转置卷积时，有2种方式可以减少伪影。一是确保滤波器大小可以被stride整除，来避免不均匀重叠问题。二是可以设置stride=1的转置卷积来减少棋盘格伪影。然而正如许多新的模型中所示，伪影仍旧可能出现。</p><p>该文（<a href="https://distill.pub/2016/deconv-checkerboard/">https://distill.pub/2016/deconv-checkerboard/</a>）进一步提出了一种更好的上采样方法来避免棋盘格伪影：首先缩放图像（使用最近邻插值或双线性差值），然后使用传统的卷积。</p><h1 id="p4-空洞卷积dilated-convolution-atrous-convolution">P4. 空洞卷积（Dilated Convolution, Atrous Convolution）</h1><p>空洞卷积见论文</p><p><a href="https://arxiv.org/abs/1412.7062">https://arxiv.org/abs/1412.7062</a></p><p><a href="https://arxiv.org/abs/1511.07122">https://arxiv.org/abs/1511.07122</a></p><p>空洞卷积在卷积核上插入0使得卷积核膨胀，在不增加参数量的情况下，增加模型感受野。pooling的下采样会导致信息丢失，是不可逆的，而空洞卷积可以替代pooling。</p><p>标准离散卷积如下：</p>\[\left( F*k \right)\left( \mathbf{p} \right)=\sum\nolimits_{\mathbf{s}+\mathbf{t}=\mathbf{p}}{F\left( \mathbf{s} \right)k\left( \mathbf{t} \right)}\]<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/4_1.gif" alt="4_1" /></p><p>空洞卷积如下：</p>\[\left( F{ {*}_{l}}k \right)\left( \mathbf{p} \right)=\sum\nolimits_{\mathbf{s}+l\mathbf{t}=\mathbf{p}}{F\left( \mathbf{s} \right)k\left( \mathbf{t} \right)}\]<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/4_2.gif" alt="4_2" /></p><p>当l=1时，空洞卷积和标准卷积相同。</p><p>空洞卷积和转置卷积的区别是：转置卷积在图像（或特征图）上插入0，空洞卷积则在卷积核上插入0来使核“膨胀”。参数l（扩张率，dilation rate）表示希望将核加宽的程度。通常在卷积核元素之间插入l-1个0。当l=1,2,4时分别如下图（空洞卷积的好处是可以在不增加额外开销的情况下增大感受野）：</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/4_3.png" alt="4_3" /></p><p>图中3*3红点代表经过卷积后，输出图像为3*3像素。虽然这些空洞卷积的结果都是3*3像素，但是感受野相差很大。l=1时感受野是3*3，l=2时感受野是7*7，l=3时感受野增大到15*15。与此同时，这三种情况下模型参数相同。因而空洞卷积可以在不增加卷积核大小的情况下，增大输出特征的感受野，在堆叠多个空洞卷积的情况下很有用。</p><p>论文“Multi-scale context aggregation by dilated convolutions”中作者使用空洞卷积建立了一个多层网络，其中扩张率l每层呈指数级增加，从而感受野随层呈指数级增加，而参数量仅线性增加。论文中使用空洞卷积在不损失分辨率的情况下聚合多尺度上下文信息（contextual information）。</p><p>空洞卷积的缺点是由于kernel的不连续，导致信息的连续性会有损失，对于密集的纹理信息处理不好。</p><h1 id="p5-扁平卷积flattened-convolutions">P5. 扁平卷积（Flattened convolutions）</h1><p>最先由“Flattened convolutional neural networks for feedforward acceleration”提出，论文为<a href="https://arxiv.org/abs/1412.5474">https://arxiv.org/abs/1412.5474</a></p><p>该卷积将标准的卷积分解成3个1D卷积，这种思想类似于空间可分卷积（spatial separable convolution），将其中的空间滤波器分为2个秩=1的滤波器。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/5.png" alt="5" /></p><p>需要注意的是，如果标准卷积核的秩=1，这些核总能分解成3个1D滤波器的叉乘（cross-products）。但是由于标准滤波器的秩总是大于1，因而该假设是一个强条件。正像文中指出的“As the difficulty of classification problem increases, the more number of leading components is required to solve the problem… Learned filters in deep networks have distributed eigenvalues and applying the separation directly to the filters results in significant information loss.”</p><p>为了缓解该问题，稳重限制感受野的连接，使得模型在训练阶段能够学习1维的可分滤波器。论文表明通过训练3D空间上各个方向连续的1D滤波器组成的扁平网络，能够在显著降低模型参数的情况下，得到和标准卷积网络相近的性能。</p><h1 id="p6-1x1pointwise-convolutions">P6. 1x1/Pointwise Convolutions</h1><p>1*1卷积最初在Network-in-network(NiN, <a href="https://arxiv.org/abs/1312.4400">https://arxiv.org/abs/1312.4400</a>)中提出，后被广泛用于Inception中（<a href="https://arxiv.org/abs/1409.4842">https://arxiv.org/abs/1409.4842</a>）。主要用于通道变换，在不改变输入特征的宽高的情况下，对通道数进行升维或者降维。卷积核大小为1*1，卷积核参数量为Cout*Cin*1*1，其中Cout个Cin*1*1的卷积核和输入特征进行乘法，得到Cout个H*W的临时结果，并将这些结果拼接，得到Cout*H*W的输出结果，在未改变特征宽高的情况下，改变了特征的通道数。</p><p>1*1卷积的优点如下：</p><ul><li><p>Dimensionality reduction for efficient computations：有效进行维度变换</p><li><p>Efficient low dimensional embedding, or feature pooling：即便低维嵌入式空间中也可能包含相对较大图像块的大量信息。因而在使用3*3或5*5卷积之前先使用1*1卷积。</p><li><p>Applying nonlinearity again after convolution：可在1*1卷积之后增加ReLU等激活函数，让网络学到更复杂的变换。</p><li><p>Yann LeCun指出全连接层充当1*1卷积：“In Convolutional Nets, there is no such thing as “fully-connected layers”. There are only convolution layers with 1x1 convolution kernels and a full connection table.”（卷积网络中没有全连接层，只有使用1*1卷积的层和全连接表）。</p></ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/6.png" alt="6" /></p><h1 id="p7-分组卷积grouped-convolution">P7. 分组卷积（Grouped Convolution）</h1><p>来自2012年的论文AlexNet（之后在ResNeXt中再次使用，主要动机是通过将特征分组，来减少计算复杂度），主要原因是在显存有限的2个GPU上（每个GPU有1.5GB显存）训练模型。如下图的AlexNet，大多数层上为2个分开的卷积路径。该网络在2个（或多个）GPU上进行模型并行化。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/7_1.png" alt="7_1" /></p><p>接下来介绍分组卷积的工作原理。首先，传统的2D卷积工如下：Hin*Win*Din的输入通过Dout个h*w*Din的卷积核变换到Hout*Wout*Dout的输出。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/7_2.png" alt="7_2" /></p><p>分组卷积中，卷积核被分成不同的组。每组在通道方向上负责特定深度的传统2D卷积，并将结果拼接，得到最终结果。如下图显示了2组的分组卷积。每组卷积核的深度为Din/2，每组滤波器包含Dout/2个卷积核。第一组（红色）用于处理输入层的前一半数据（[:, :, 0:Din/2]），第二组（橙色）用于处理输入层的后一半数据（[:, :, Din/2:Din]）。因而每组会输出Dout/2个通道。两组结果拼接起来，得到Dout通道的输出。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/7_3.png" alt="7_3" /></p><p>分组卷积和深度可分离卷积中的深度卷积的关系：如果分组卷积中的滤波器组数和输入层通道数相同（每个滤波器深度为1），等价于深度卷积；如果每个滤波器组包含Dout/Din个滤波器（输出层深度为Dout），这和深度卷积不同：深度卷积不改变层的深度（深度可分离卷积中，通过1*1卷积实现通道变换）。</p><p>分组卷积的优点：</p><p><strong>① 训练更高效</strong>：每次能使用更多图像训练。模型并行化比数据并行化更有效。</p><p><strong>② 模型更高效</strong>：模型参数随着分组数量的增加而降低。假定分组数为n，则模型包含 \(\left( h*w*\frac{ { {D}_{in}}}{n}*\frac{ { {D}_{out}}}{n} \right)*n\) 个参数，参数数量降低到原来的 \(\frac{1}{n}\) 。</p><p><strong>③ 分组卷积能提供更好的模型</strong>。具体见：<a href="https://blog.yani.io/filter-group-tutorial/">https://blog.yani.io/filter-group-tutorial/</a>。简单来说和系数滤波器的关系有关。该博客提出了一个原因：The effect of filter groups is to learn with a block-diagonal structured sparsity on the channel dimension… the filters with high correlation are learned in a more structured way in the networks with filter groups. In effect, filter relationships that don’t have to be learned are no longer parameterized. In reducing the number of parameters in the network in this salient way, it is not as easy to over-fit, and hence a regularization-like effect allows the optimizer to learn more accurate, more efficient deep networks.”（滤波器组用于学习在通道维度上对角块结构的洗属性…在有滤波器组的网络中，能以更结构化的方式学习具有更高相关性的滤波器。从效果上看，无需参数化那些不必学习的滤波器关系。通过这种方式显著减少网络参数量时，不易过拟合，且这种类似正则化的方式允许优化器学习更准确、更有效的深度网络。）另一方面，每组滤波器学习了不同性质的滤波器：黑白滤波器和彩色滤波器，如下图：</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/7_4.png" alt="7_4" /></p><p>分组卷积可以极大减少模型参数，同时增大卷积之间的对角相关性，不容易过拟合，相当于正则的效果。</p><h1 id="p8-混淆分组卷积shuffled-grouped-convolution">P8. 混淆分组卷积（Shuffled Grouped Convolution）</h1><p>由旷视在<a href="https://arxiv.org/abs/1707.01083">https://arxiv.org/abs/1707.01083</a>中提出（ShuffleNet）。ShuffleNet可用于算力有限的移动设备上(如10–150 MFLOPs)。混淆分组卷积基于分组卷积和宽度可分离卷积（depthwise separable convolution），其包括分组卷积核通道混淆。</p><p>下图为有2个堆叠分组卷积的通道混淆示意图。GConv代表分组卷积。a为分组数相同的2个堆叠的分组卷积，由于每组只负责处理前一层传进来的信息因而每个滤波器组只能学习特定的特征，这种方式阻断了不同组之间的信息交换，削弱了模型在训练阶段的表达能力。因而使用通道混淆，来混合不同滤波器组的信息，如b所示，先将GConv1的特征分成不同的子组并组合，然后送入分组卷积GConv2，得到最终的特征。c为等效于b的实现，即先对GConv1输出的特征进行通道混淆，然后再正常输入GConv2。分组卷积中，每个滤波器组学习当前输入的特定特征，阻碍了特征信息在不同组之间的传播，通道混淆则打乱了通道顺序，解决了信息无法传播的问题。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/8.png" alt="8" /></p><h1 id="p9-pointwise-grouped-convolution逐点分组卷积">P9. Pointwise grouped convolution逐点分组卷积</h1><p>ShuffleNet（<a href="https://arxiv.org/abs/1707.01083">https://arxiv.org/abs/1707.01083</a>）中还提出了逐点分组卷积（pointwise grouped convolution）。MobileNet或ResNeXt中在3*3卷积中使用分组卷积，而未在1*1卷积中使用。ShuffleNet认为1*1卷积计算成本也很高，因而建议1*1卷积也使用分组卷积。1*1的分组卷积称作逐点分组卷积（pointwise grouped convolution），能进一步降低计算成本。</p><p>文中提出了ShuffleNet单元，如下图。a为使用宽度卷积（depthwise convolution， DWConv）的瓶颈单元。b为使用逐点分组卷积（pointwise group convolution）和通道混淆的ShuffleNet单元。c为当stride=2时的ShuffleNet单元，其中将最后的操作由add改为了concat。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/9.png" alt="9" /></p><h1 id="p10-spatial-and-cross-channel-convolutions空间和跨通道卷积">P10. Spatial and Cross-Channel Convolutions空间和跨通道卷积</h1><p>Inception网络中广泛使用。主要将跨通道的相关性和空间相关性拆分成一系列独立的操作。空间相关性指在宽、高方向上使用卷积，如下图的3*3卷积。如下图，使用3个独立的“1*1卷积+3*3卷积”分别处理通道相关性。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/10.png" alt="10" /></p><h1 id="p11-可分卷积separable-convolutions">P11. 可分卷积（Separable Convolutions）</h1><p>可分卷积在MobileNet（<a href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</a>）等网络中使用，其包括空间可分卷积（Spatially Separable Convolutions）和深度可分卷积（Depthwise Separable Convolutions）</p><h2 id="p111-空间可分卷积-spatially-separable-convolutions">P11.1. 空间可分卷积 Spatially Separable Convolutions</h2><p>空间可分卷积在图像（或特征）的宽高（WH）维度上进行卷积。从概念上来说，空间可分离卷积将一个卷积分解为两个单独的运算。如3*3的Sobel核可被分解成1个3*1的核和一个1*3的核，如下图。传统卷积中，3*3的核直接和图像卷积。空间可分离卷积中，先将3*1的核和图像卷积，再将1*3的核和图像卷积。在执行相同操作的情况下，将参数量从9降低到6。</p>\[\left[ \begin{matrix} \text{-}1 &amp; 0 &amp; 1 \\ \text{-}2 &amp; 0 &amp; 2 \\ \text{-}1 &amp; 0 &amp; 1 \\ \end{matrix} \right]\text{=}\left[ \begin{matrix} 1 \\ 2 \\ 1 \\ \end{matrix} \right]\times \left[ \begin{matrix} \text{-}1 &amp; 0 &amp; 1 \\ \end{matrix} \right]\]<p>另一方面，空间可分离卷积也可以减少矩阵乘法次数。对于N*N的单通道图像和m*m的卷积核，在stride=1，padding=0时，传统卷积需要(N-2)*(N-2)*m*m次乘法，空间可分离卷积需要N*(N-2)*m+(N-2)*(N-2)*m=(2N-2) x (N-2) x m次乘法。空间可分离卷积和标准卷积的计成本之比为：</p>\[\frac{2}{m}+\frac{2}{m(N-2)}\]<p>当图像宽高N远大于卷积核大小m（N » m）时，该比例近似为2/m。 虽然空间可分离卷积节省计算代价，但是在深度学习中很少使用。一个主要原因是不是所有卷积核都能被分解成2个更小的卷积核。若将传统卷积替换为空间可分离卷积，在训练中会限制搜索到的所有可能的卷积核。导致训练得到次优模型。</p><h2 id="p112-深度可分离卷积-depthwise-separable-convolutions">P11.2. 深度可分离卷积 Depthwise Separable Convolutions</h2><p>深度可分离卷积在深度学习中更常使用，如MobileNet and Xception。深度可分离卷积包括2步：深度卷积（depthwise convolutions）和1*1卷积。</p><p>深度可分离卷积可见：<a href="https://www.cnblogs.com/darkknightzh/p/9410540.html">https://www.cnblogs.com/darkknightzh/p/9410540.html</a></p><p>在对 \(\left[ { {C}_{in}},{ {H}_{in}},{ {W}_{in}} \right]\) 的特征进行传统卷积时（stride=1，padding=0），卷积核共 \({ {C}_{out}}*{ {C}_{in}}*{ {H}_{k}}*{ {W}_{k}}\) 个参数，为 \({ {C}_{out}}\) 个 \(\left[ { {C}_{in}},{ {H}_{k}},{ {W}_{k}} \right]\) 的卷积核和输入特征做卷积，得到 \({ {C}_{out}}\) 个 \(\left[ 1,{ {H}_{in}}-2,{ {W}_{in}}-2 \right]\) 的临时结果；并将 \({ {C}_{out}}\) 个临时结果拼接成 \(\left[ { {C}_{out}},{ {H}_{in}}-2,{ {W}_{in}}-2 \right]\) 的输出特征。</p><p>在对 \(\left[ { {C}_{in}},{ {H}_{in}},{ {W}_{in}} \right]\) 的特征进行深度可分离卷积时（stride=1，padding=0），首先使用 \({ {C}_{in}}\) 个 \(\left[ 1,{ {H}_{k}},{ {W}_{k}} \right]\) 的卷积核和输入的 \({ {C}_{in}}\) 个 \(\left[ 1,{ {H}_{in}},{ {W}_{in}} \right]\) 的特征分别进行卷积，得到 \({ {C}_{in}}\) 个临时结果，并将这 \({ {C}_{in}}\) 个临时结果拼接，得到 \(\left[ { {C}_{in}},{ {H}_{in}}-2,{ {W}_{in}}-2 \right]\) 的临时结果2。此时通道数（特征的深度）不变。之后使用 \({ {C}_{out}}\) 个 \(\left[ { {C}_{in}},1,1 \right]\) 的卷积核，和临时结果2分别进行卷积，得到 \({ {C}_{out}}\) 个 \(\left[ 1,{ {H}_{in}}-2,{ {W}_{in}}-2 \right]\) 的结果，并在通道维度拼接，得到 \(\left[ { {C}_{out}},{ {H}_{in}}-2,{ {W}_{in}}-2 \right]\) 的输出，该结果维度和传统卷积维度一样。</p><p>深度可分离卷积的优点：高效。与2D卷积相比，深度可分离卷积需要更少的操作。</p><p>对于 \(\left[ { {C}_{in}},{ {H}_{in}},{ {W}_{in}} \right]\) 的特征，stride=1，padding=0时，传统卷积的乘法次数：</p>\[{ {C}_{out}}\times { {C}_{in}}\times { {H}_{k}}\times { {W}_{k}}\times \left( { {H}_{in}}-{ {H}_{k}}+1 \right)\times \left( { {W}_{in}}-{ {W}_{k}}+1 \right)\]<p>深度卷积乘法次数： \({ {C}_{in}}\times { {H}_{k}}\times { {W}_{k}}\times 1\times \left( { {H}_{in}}-{ {H}_{k}}+1 \right)\times \left( { {W}_{in}}-{ {W}_{k}}+1 \right)\) ，1*1卷积乘法次数： \({ {C}_{in}}\times 1\times 1\times { {C}_{out}}\times \left( { {H}_{in}}-{ {H}_{k}}+1 \right)\times \left( { {W}_{in}}-{ {W}_{k}}+1 \right)\) ，因而深度可分离卷积乘法次数：</p>\[{ {C}_{in}}\times { {H}_{k}}\times { {W}_{k}}\times 1\times \left( { {H}_{in}}-{ {H}_{k}}+1 \right)\times \left( { {W}_{in}}-{ {W}_{k}}+1 \right)+{ {C}_{in}}\times 1\times 1\times { {C}_{out}}\times \left( { {H}_{in}}-{ {H}_{k}}+1 \right)\times \left( { {W}_{in}}-{ {W}_{k}}+1 \right)\]<p>深度可分离卷积和原始卷积乘法次数之比为：</p>\[\frac{1}{ { {C}_{out}}}+\frac{1}{ { {H}_{k}}*{ {W}_{k}}}\]<p>当输出通道数Cout远大于卷积核宽高时，上式近似为 \(1/\left( { {H}_{k}}\times { {W}_{k}} \right)\) ，意味着当时用3*3卷积核时，2D卷积乘法次数为深度可分离卷积的9倍。</p><p>深度可分离卷积的缺点：深度可分离卷积降低了模型的参数，在模型比较小时，相比2D卷积的性能可能会降低的很明显，导致模型模型次优。但是，如果使用恰当的话，深度可分离卷积能在不显著降低模型性能的前提下，提高模型效率。</p><p><strong>Inception模块和可分卷积的区别</strong>：a 可分卷积先在通道上使用空间卷积，然后使用1*1conv。Inception模块先使用1*1conv。b 可分卷积通常不使用非线性层。</p><h1 id="p12-可变性卷积deformable-convolution">P12. 可变性卷积（Deformable Convolution）</h1><p>可变形卷积是在基础卷积核的上添加一些位移量，根据数据的学习情况，自动调整偏移，卷积核可以在任意方向进行伸缩，改变感受野的范围，该位移量是靠额外的一个卷积层进行自动学习的，如下图，（a）是普通的卷积，卷积核大小为3*3，采样点排列非常规则，是一个正方形。（b）是可变形的卷积，给每个采样点加一个offset（这个offset通过额外的卷积层学习得到），排列变得不规则。（c）和（d）是可变形卷积的两种特例。对于（c）加上offset，达到尺度变换的效果；对于（d）加上offset，达到旋转变换的效果。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/12_1.png" alt="12_1" /></p><p>某一点P0经过标准卷积后结果为</p>\[y\left( { {P}_{0}} \right)=\sum\limits_{ { {P}_{n}}\in R}{w\left( { {P}_{n}} \right)\centerdot x\left( { {P}_{0}}+{ {P}_{n}} \right)}\]<p>而经过可变形卷积后的结果为：</p>\[y\left( { {P}_{0}} \right)=\sum\limits_{ { {P}_{n}}\in R}{w\left( { {P}_{n}} \right)\centerdot x\left( { {P}_{m}} \right)}\]<p>这里 \({ {P}_{m}}={ {P}_{0}}+{ {P}_{n}}+\Delta { {P}_{n}}\) ， \(\Delta { {P}_{n}}\) 为卷积核的偏移量offset。可变形卷积更能适应目标的各种形变。</p><p>就特征提取的形状而言，卷积非常严格。也就是说，kernel形状仅为正方形/矩形（或其他一些需要手动确定的形状），因此它们只能在这种模式下使用。如果卷积的形状本身是可学习的呢？这是引入可变形卷积背后的核心思想。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/12_2.jpg" alt="12_2" /></p><p>实际上，可变形卷积的实现非常简单。每个kernel都用两个不同的矩阵表示。第一分支学习从原点预测“偏移”。此偏移量表示要处理原点周围的哪些输入。由于每个偏移量都是独立预测的，它们之间无需形成任何刚性形状，因此具有可变形的特性。第二个分支只是卷积分支，其输入是这些偏移量处的值。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/post/2021-07-23-differentconv/12_3.jpg" alt="12_3" /></p></div><div class="post-tail-wrapper text-muted"><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div class="post-tags"> <span>标签(Tags)</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >deep learning</a> <a href="/tags/algorithm/" class="post-tag no-text-decoration" >algorithm</a></div></div><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/UbuntuAgent/">Ubuntu使用代理</a><li><a href="/posts/SunloginDisconnectWin10/">windows10使用向日葵访问ubuntu 20.04显示连接已断开</a><li><a href="/posts/TimeWin10Ubuntu/">windows10和ubuntu双系统的时间差</a><li><a href="/posts/markdown/">markdown基本语法</a><li><a href="/posts/MountDriverInWin10Ubuntu/">windows10的ubuntu子系统挂载移动硬盘</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/algorithm/">algorithm</a> <a class="post-tag" href="/tags/deep-learning/">deep learning</a> <a class="post-tag" href="/tags/detection/">detection</a> <a class="post-tag" href="/tags/normalization/">normalization</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/transformers/">transformers</a> <a class="post-tag" href="/tags/demo/">demo</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h6 align="left"><br>未设置提醒，因而不一定能看到回复，见谅</h6><div class="post-navigation d-flex justify-content-between"> <a href="/posts/backpropPool/" class="btn btn-outline-primary" prompt="Older"><p>池化反向传播公式的推导</p></a> <a href="/posts/batchinstancenorm/" class="btn btn-outline-primary" prompt="Newer"><p>Batch-Instance Normalization（BIN）</p></a></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script> <script src="https://code.jquery.com/jquery-3.2.0.min.js"></script> <script>AV.initialize("7DUQTBuCCKLjnutJOa6ko5cn-MdYXbMMI", "lxmthTQ8ESa2HrVQiIVyXtYo");</script> <script> //新增访问次数 function addCount(Counter) { // 页面（博客文章）中的信息：leancloud_visitors // id为page.url， data-flag-title为page.title var $visitors = $(".leancloud_visitors"); var url = $visitors.attr('id').trim(); var title = $visitors.attr('data-flag-title').trim(); var query = new AV.Query(Counter); // 只根据文章的url查询LeanCloud服务器中的数据 query.equalTo("post_url", url); query.find({ success: function(results) { if (results.length > 0) {//说明LeanCloud中已经记录了这篇文章 var counter = results[0]; counter.fetchWhenSave(true); counter.increment("visited_times");// 将点击次数加1 counter.save(null, { success: function(counter) { var $element = $(document.getElementById(url)); var newTimes = counter.get('visited_times'); $element.find('.leancloud-visitors-count').text(newTimes); }, error: function(counter, error) { console.log('Failed to save Visitor num, with error message: ' + error.message); } }); } else { // 执行这里，说明LeanCloud中还没有记录此文章 var newcounter = new Counter(); /* Set ACL */ var acl = new AV.ACL(); acl.setPublicReadAccess(true); acl.setPublicWriteAccess(true); newcounter.setACL(acl); /* End Set ACL */ newcounter.set("post_title", title);// 把文章标题 newcounter.set("post_url", url); // 文章url newcounter.set("visited_times", 1); // 初始点击次数：1次 newcounter.save(null, { // 上传到LeanCloud服务器中 success: function(newcounter) { var $element = $(document.getElementById(url)); var newTimes = newcounter.get('visited_times'); $element.find('.leancloud-visitors-count').text(newTimes); }, error: function(newcounter, error) { console.log('Failed to create'); } }); } }, error: function(error) { console.log('Error:' + error.code + " " + error.message); } }); } //仅根据url和title查出当前访问次数，不做+1操作 function showCount(Counter) { var $visitors = $(".leancloud_visitors"); var url = $visitors.attr('id').trim(); var title = $visitors.attr('data-flag-title').trim(); var query = new AV.Query(Counter); // 只根据文章的url查询LeanCloud服务器中的数据 query.equalTo("post_url", url); query.find({ success: function(results) { if (results.length > 0) {//说明LeanCloud中已经记录了这篇文章 var counter = results[0]; var $element = $(document.getElementById(url)); var newTimes = counter.get('visited_times'); $element.find('.leancloud-visitors-count').text(newTimes); } else { //如果表里没查到记录，那就是异常情况了 console.log('异常情况，不应该没记录的'); } }, error: function(error) { console.log('Error:' + error.code + " " + error.message); } }); } //调用API获取IP function getVisitorIpAndJudge() { var ip; var options = { type: 'POST', dataType: "json", //async: false, //jquery3中可以直接使用回调函数，不用再指定async url: "https://freegeoip.net/json/?callback=?" }; $.ajax(options) .done(function(data, textStatus, jqXHR) { if(textStatus == "success") { ip = data.ip; } judgeVisitor(ip) }); } //判断访客是否已访问过该文章，及访问时间，符合条件则增加一次访问次数 function judgeVisitor(ip) { var Counter = AV.Object.extend("visited_times"); var Visitor = AV.Object.extend("visitors_record"); var $postInfo = $(".leancloud_visitors"); var post_url = $postInfo.attr('id').trim(); var query = new AV.Query(Visitor); query.equalTo("visitor_ip", ip); query.equalTo("post_url", post_url); query.find({ success: function(results) { if (results.length > 0) { console.log('该IP已访问过该文章'); var oldVisitor = results[0]; var lastTime = oldVisitor.updatedAt; var curTime = new Date(); var timePassed = curTime.getTime() - lastTime.getTime(); if(timePassed > 1 * 60 * 1000) { console.log('距离该IP上一次访问该文章已超过了1分钟，更新访问记录，并增加访问次数'); addCount(Counter); oldVisitor.fetchWhenSave(true); oldVisitor.save(null, { success: function(oldVisitor) { }, error: function(oldVisitor, error) { console.log('Failed to save visitor record, with error message: ' + error.message); } }); } else { console.log('这是该IP 1分钟内重复访问该文章，不更新访问记录，不增加访问次数'); showCount(Counter); } } else { console.log('该IP第一次访问该文章，保存新的访问记录，并增加访问次数'); addCount(Counter); var newVisitor = new Visitor(); /* Set ACL */ var acl = new AV.ACL(); acl.setPublicReadAccess(true); acl.setPublicWriteAccess(true); newVisitor.setACL(acl); newVisitor.set("visitor_ip", ip); newVisitor.set("post_url", post_url); newVisitor.save(null, { // 上传到LeanCloud服务器中 success: function(newVisitor) { }, error: function(newVisitor, error) { console.log('Failed to create visitor record, with error message: ' + error.message); } }); } }, error: function(error) { console.log('Error:' + error.code + " " + error.message); addCount(Counter); } }); } $(function() { if ($('.leancloud_visitors').length == 1) { // 文章页面，调用判断方法，对符合条件的访问增加访问次数 getVisitorIpAndJudge(); } else if ($('.post-link').length > 1){ // 首页 暂未使用 // showHitCount(Counter); } }); </script><div> <span id="/posts/differentconv/" class="leancloud_visitors" data-flag-title="深度学习中的各种卷积"> <a href="#">Pageviews:<span class="leancloud-visitors-count"></span> times</a></span></div><h4 align="left">用户留言：</h4><div id="comments"></div><script src='//unpkg.com/valine/dist/Valine.min.js'></script> <script> new Valine({ av: AV, el: '#comments', app_id: '7DUQTBuCCKLjnutJOa6ko5cn-MdYXbMMI', app_key: 'lxmthTQ8ESa2HrVQiIVyXtYo', placeholder: '', avatar: 'mp', notify: 'true', verify: 'true', recordIP: 'true', enableQQ: 'true', visitor: true }); </script></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#main > div.row:first-child > div:first-child img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2025 <a href="">darkknightzh</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/algorithm/">algorithm</a> <a class="post-tag" href="/tags/deep-learning/">deep learning</a> <a class="post-tag" href="/tags/detection/">detection</a> <a class="post-tag" href="/tags/normalization/">normalization</a> <a class="post-tag" href="/tags/linux/">linux</a> <a class="post-tag" href="/tags/transformers/">transformers</a> <a class="post-tag" href="/tags/demo/">demo</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://darkknightzh.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
