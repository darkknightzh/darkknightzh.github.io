---
layout: post
title:  "YOLOV2 YOLO9000: Better, Faster, Stronger"
date:   2021-08-26 16:00:00 +0800
tags: [deep learning, algorithm, detection]
pin: true
math: true
---

<style> h1 { border-bottom: none } </style>

转载请注明出处：

<https://darkknightzh.github.io/posts/YOLOV2>

论文：

<https://arxiv.org/abs/1612.08242>

和Fast R-CNN相比，YOLO目标定位错误多很多。和基于候选区域的方法相比，YOLO召回率低。因而该文提出YOLOv2和YOLO9000，在保持分类精度的前提下，提高召回率和定位精度。采用Darknet-19的骨干网络，使用呢BatchNorm，并分别使用224\*224和448\*448两阶段ImageNet预训练模型进行微调。引入anchor机制，使用K-Means在训练集中积累出更好的anchor模板，提高了召回率。结合图像细粒度特征，见浅层特征与深层特征连通，对小尺寸目标检测效果更好。

该文借鉴了之前的研究成果：


## **BatchNorm**

BN能够加速收敛。在卷积层之后增加BN层，mAP能够提高2%。使用BN并去除dropout层。


## **高分辨率分类**

该文首先在ImageNet上使用448\*448的分辨率微调分类网络10epochs。然后微调检测网络。mAP增加4%。


## **使用卷积层预测anchor信息**

YOLO在最后使用全连接层直接预测边界框的坐标。而Faster R-CNN使用RPN网络预测边界框偏移和置信度。由于预测层也是卷积，因而RPN网络输出的特征图为每个位置的偏移。预测偏移取代预测坐标后，网络更容易学习。

因而该文去除了YOLO中的全连接层，使用anchor box预测边界框。首先去除一个池化层，此时网络输出分辨率更高。而后将网络输入从448\*448降低到416\*416，这样可以保证最终得到的特征图为奇数尺寸。大目标倾向于占据图像的中心，因而最好在特征图中心有一个位置，方便预测这些目标（当特征图尺寸为偶数时，此时目标中心为特征图上四个相邻的位置）。YOLO卷积层下采样率为32，因而图像为416\*416时，特征图尺寸为13\*13。

使用anchor框后，修改了预测机制。预测每个anchor box的类别（预测假定该处是目标的条件下该目标类别的条件概率）和objectness（GT框和候选框的IOU，即其是目标的分数，因为和GT框的IOU越大，其是目标的可能性越大）。

使用anchor后，mAP从69.5%降低到69.2，但是召回率从81%增加到88%。


## **k-means确定候选框形状（Dimension Clusters）**

使用anchor是遇到2个问题。一个是框的维度需要手工挑选。网络可以通过学习来调整边界框，但如果我们开始便给网络提供选择更好的先验知识，网络检测效果会更好。

因而该文使用k-means对训练集边界框进行聚类，来找到边界框的先验知识。如果使用标准k-means结合欧式距离的话，大的框会有更大的误差。而该文需要能提高IOU、且独立于目标框大小的先验知识，因而使用如下的测度：

$$d\left( \text{box,centroid} \right)=1-\text{IOU}\left( \text{box,centroid} \right)$$

图1显示了IOU和不同k的关系。因而选择k=5来均衡模型复杂度和召回率。聚类中心和手工选择的anchor框明显不同。聚类得到的框矮、宽的框较少，而高瘦的框更多。使用5个聚类中心的IOU（61%）和9个anchor的IOU（60.9%）相近，而使用9个聚类中心的IOU（67.2%）显著增加。

![1](/assets/post/2021-08-26-YOLOV2/1clustering.png)
_图1_

直接预测位置（预测目标相对于左上角网格的偏移）：使用YOLO结合anchor时，第二个问题是模型不稳定，特别是训练初期。大多数不稳定来源于预测框的(x, y)坐标。基于候选区域的模型预测
$${ {t}_{x}}$$
和
$${ {t}_{y}}$$
，因而预测框的(x, y)重心坐标为

$$x=\left( { {t}_{x}}*{ {w}_{a}} \right)-{ {x}_{a}}$$

$$y=\left( { {t}_{y}}*{ {h}_{a}} \right)-{ {y}_{a}}$$

比如
$${ {t}_{x}}=1$$
会将anchor框向右偏移一个宽度，
${ {t}_{x}}=\text{-}1$$
会将anchor框向左偏移一个宽度。由于没有约束，因而任何anchor框都可以在图像上的任何位置。随机初始化模型会消耗很长时间让模型学习到有意义的偏移。

因而该文使用YOLO的方式，不去预测偏移，而是预测目标和网格的相对坐标，并使用logistic activation限制GT的相对坐标在0-1之间。

网络的每个网格的输出特征图上预测边界框的5个参数：
$${ {t}_{x}}$$
，
$${ {t}_{y}}$$
，
$${ {t}_{w}}$$
，
$${ {t}_{h}}$$
，
$${ {t}_{o}}$$
。如果网格偏离图像左上角
$$\left( { {c}_{x}},{ {c}_{y}} \right)$$
，边界框的先验宽高为
$${ {p}_{w}}$$
，
$${ {p}_{h}}$$
，则预测结果如下。示例如图2所示。

$${ {b}_{x}}=\sigma \left( { {t}_{x}} \right)+{ {c}_{x}}$$

$${ {b}_{y}}=\sigma \left( { {t}_{y}} \right)+{ {c}_{y}}$$

$${ {b}_{w}}={ {p}_{w}}{ {e}^{ { {t}_{w}}}}$$

$${ {b}_{h}}={ {p}_{h}}{ {e}^{ { {t}_{h}}}}$$

$$Pr\left( \text{object} \right)*IOU\left( b,\text{object} \right)=\sigma \left( { {t}_{o}} \right)$$

其中
$${ {t}_{x}}$$
，
$${ {t}_{y}}$$
，
$${ {t}_{w}}$$
，
$${ {t}_{h}}$$
，
$${ {t}_{o}}$$
为模型要预测的值，
$${ {b}_{x}}$$
，
$${ {b}_{y}}$$
，
$${ {b}_{w}}$$
，
$${ {b}_{h}}$$
，
$${ {b}_{o}}$$
为模型最终得到的检测结果。
$${ {c}_{x}}$$
，
$${ {c}_{y}}$$
为目标中心所在网格距离图像左上角的位置，
$${ {p}_{w}}$$
，
$${ {p}_{h}}$$
为之前确定的anchor的先验宽高。
 
![2](/assets/post/2021-08-26-YOLOV2/2prediction.png)
_图2_


## **精细纹理特征（Fine-Grained Features）**

本文方法在13\*13的特征图上预测结果。适合检测大目标，如果使用更精细的纹理特征，能更有效地检测小目标。Faster R-CNN和SSD等在不同特征图上使用候选框，来得到多分辨率特征。该文换了一种思路，简单的将网络的26\*26的特征通过直通层（passthrough layer）拼接到13\*13的特征上。直通层将高分辨率的26\*26\*512特征的相邻特征放到不同通道，得到13\*13\*2048的特征，并和低分辨率特征拼接，类似于ResNet的等同映射。这样能提高1%的性能。


## **多尺度训练**

原始YOLO使用448\*448的图像进行训练。本文为了方便使用anchor，将输入分辨率降低到416\*416。为了使用多尺度训练（输出分辨率为下采样率32的整数倍），本文每个10batches，从{320, 352, …, 608}中随机选择输入图像分辨率，并训练模型。这样网络可以检测不同分辨率的目标。


## **Darknet-19**

为了速度更快，该文提出了新的骨干网络Darknet-19，如图3所示。其使用3\*3卷积，并在每个池化层后将通道数增加一倍，另外使用全局池化进行预测，在3\*3卷积之间使用1\*1卷积压缩特征（特征通道数降低一半）。其包括19个卷积层，5个maxpool层。

![3](/assets/post/2021-08-26-YOLOV2/3details.png)
_图3_

说明：接下来还有利用了标签的层级结构的YOLO9000，没看。
