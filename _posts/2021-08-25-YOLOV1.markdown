---
layout: post
title:  "YOLOV1 You Only Look Once: Unified, Real-Time Object Detection"
date:   2021-08-25 16:00:00 +0800
tags: [deep learning, algorithm, detection]
pin: true
math: true
---

<style> h1 { border-bottom: none } </style>

转载请注明出处：

<https://darkknightzh.github.io/posts/YOLOV1>

论文：

<https://arxiv.org/abs/1506.02640>

YOLO是深度学习的第一个一阶段检测器。其将单个神经网络应用到整个图像，将图像调整到448\*448，分割成7\*7的网格，提取特征，直接预测每个网格的边界框坐标和每个类别的置信度（有什么目标，是这个目标的置信度）。训练时使用leaky ReLU激活函数，测试时使用NMS去除重叠的框。

该文提出YOLO（YOLOv1），该系统将输入图像分成S\*S的网格。如果目标中心在某个网格中，该网格用于检测该目标。

每个网格预测B个**边界框和这些边界框的置信度**（是否有目标的置信度）。置信度反映了该框包含目标的置信度以及该框是相应目标的准确度。置信度定义为
$$\text{Pr}\left( \text{Object} \right)\text{*IOU}_{\text{pred}}^{\text{truth}}$$
。其中如果有目标落在一个网格里，第一项取1，否则取0。 第二项是预测框和GT框的IoU。

每个边界框包含5个预测参数：x, y, w, h, confidence。(x, y)坐标代表框的中心相对于网格边界的距离（The (x; y) coordinates represent the center of the box relative to the bounds of the grid cell）。宽和高为相对于整个图像的值。预测置信度为预测框和任何GT框的IOU。

每个网格还会预测C个**类别的条件概率**
$$\text{Pr}\left( \text{Clas}{ {\text{s}}_{i}}\left| \text{Object} \right. \right)$$
。这些概率建立在网格有目标的前提下（即该网格有目标的前提下，其目标是每个类别的概率）。无论框B有多少个，本文只预测每个网格包含一个类别的概率。

每个格子需要输出5B+C 维度的向量，模型最终的输出的tensor为S\*S\*(5B+C)。由于一个网格有2个box，因而条件概率是针对每个网格的（每个网格对应哪个类别），置信度是针对每个边界框的（当前网格中每个边界框的置信度）。

测试阶段，将条件类别概率和框预测置信度相乘：

$$\text{Pr}\left( \text{Clas}{ {\text{s}}_{i}}\left| \text{Object} \right. \right)*\text{Pr}\left( \text{Object} \right)\text{*IOU}_{\text{pred}}^{\text{truth}}=\text{Pr}\left( \text{Clas}{ {\text{s}}_{i}} \right)*\text{IOU}_{\text{pred}}^{\text{truth}} \tag{1}$$

该公式为每个框特定类别的置信度得分。这些分数代表了该类目标出现在框中的概率以及预测框与目标的匹配程。
对于PASCAL VOC数据库，S=7，B=2。该数据库共20个目标类别，因而C=20。模型最终输出为7\*7\*30的张量。如图1所示。

![1](/assets/post/2021-08-25-YOLOV1/1model.png)
_图1 模型_

## P1. 网络结构

YOLO网络结构如图2所示。包括24个卷积层和2个全连接层。同时本文也训练了快速YOLO模型，用于快速进行目标检测，其使用9个卷积层，每层使用更少的滤波器。除了模型大小的区别，YOLO和快速YOLO其他的训练和测试参数都相同。

模型最终输出为7\*7\*30的张量。
 
![2](/assets/post/2021-08-25-YOLOV1/2architecture.png)
_图2 网络结构_

## P2. 训练

本文使用在Imagenet上预训练模型。预训练时，使用图2中的前20个卷积层，加上平均池化层和一个全连接层。预训练结束之后，增加了随机初始化的4个卷积层和2个全连接层。训练时，将输入分辨率从224\*224增加到448\*448（预训练时使用224\*224输入，训练检测器时使用448\*448输入）。

模型最后一层预测分类概率和边界框坐标。边界框宽高使用图像宽高归一化到0到1之间。将边界框的x和y坐标定义为距离特定网格的坐标偏移，因而他们也在0到1之间。

本文使用LeakyReLU作为激活函数：

$$\phi \left( x \right)=\left\{ \begin{matrix}
   x & if\text{ }x>0  \\
   0.1x & otherwise  \\
\end{matrix} \right. \tag{2}$$

本文损失函数如下：

![3](/assets/post/2021-08-25-YOLOV1/3loss.png)
_(3)_

其中
$$\mathbb{1}_{i}^{\text{obj}}$$
代表目标出现在网格i中，
$$\mathbb{1}_{ij}^{\text{obj}}$$
代表网格i中的第j个边界框负责该预测。
$${ {\lambda }_{coord}}=5$$
，
$${ {\lambda }_{noobj}}=5$$
。由于大目标较小偏离时比小目标较小偏离时对结果的影响更小，因而公式3中预测宽高时使用开方，而没有直接使用宽高。

YOLO每个网格预测多个边界框。训练阶段通过查找预测器和GT框最高的IOU来让边界框预测器负责一个目标。这导致边界框预测器的专门化，每个预测器能更好地预测特定的尺寸、宽高比、类别的目标，提高了整体召回率。

需要注意的是，只有目标在一个网格里面时，损失函数才会惩罚分类错误的目标。另外也只有预测器负责相应的GT框时，损失函数会乘法边界框坐标错误。

为了避免过拟合，本文使用dropout和额外的数据扰动策略（随机缩放、变换到至多原始图像尺寸的20%，在HSV空间中随机调整曝光、饱和度到至多原来的1.5倍）。

## P3. 推断

通常网络仅预测一个目标。但有事一些大目标或者在多个网格边界的目标能被多个网格定位。此时需要使用NMS抑制过多的检测。NMS能让mAP提高2-3%。

## P4. 缺陷

由于每个网格只预测2个边界框及一个类别，这限制了模型预测的目标数量。 且模型对小目标预测效果不佳。模型对数据库中没有出现过的目标或者宽高比不常见的目标检测效果不好。
由于损失函数对大目标和小目标的错误同等看到，但大目标的定位错误对IOU影响较小，而小目标的定位错误对IOU影响很大。
