---
layout: post
title:  "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
date:   2021-09-20 16:00:00 +0800
tags: [deep learning, algorithm, detection]
pin: true
math: true
---

<style> h1 { border-bottom: none } </style>

转载请注明出处：

<https://darkknightzh.github.io/posts/fasterRCNN>


论文：
<https://arxiv.org/abs/1506.01497>

之前的第三方代码解释：

<https://www.cnblogs.com/darkknightzh/p/10043864.html>

<https://darkknightzh.github.io/posts/fasterRCNNcode>

<br>

## P1. 简介

Faster R-CNN包括两个模块：用于提供候选区域的FCN网络（称作RPN，Region Proposal Network），使用该候选区域的Fast R-CNN检测器。整体网络结构如图1所示。RPN模块类似注意力机制，告诉Fast R-CNN模块需要向哪看。

![1](/assets/post/2021-09-20-fasterRCNN/1fasterrcnn.png)
_图1_


## P2. Region Proposal Networks

RPN输入图像，输出一系列包含目标得分的矩形候选目标。该模块使用FCN。需要注意的是，FCN和Fast R-CNN会共享一部分卷积层。如使用Zeiler and Fergus的ZF模型，则有5个共享的卷积层；使用Simonyan and Zisserman的VGG-16模型，则有13个共享的卷积层。

将一个小的网络在上述共享卷积层的最后一层上滑动，从而得到候选区域。该小网络将输入n\*n的候选窗的特征映射到256d（ZF模型）或者512d（vgg模型）的低维特征。低维特征送入2个FC网络：回归网络（reg，用来拟合目标框）和分类网络（cls，用来对目标框进行分类）。该文使用n=3。这样输入图像上的感受野比较大（ZF模型为171像素，vgg模型为228像素）。如图2中左侧的子图所示。由于该小网络使用滑动窗口的形式，因而所有空间位置共享相同的FC层。可以使用一个n\*n的卷积层加上两个独立的1\*1卷积（分别用于reg和cls）来实现该FC层。

![2](/assets/post/2021-09-20-fasterRCNN/2RPN.png)
_图2_

### P2.1 anchors

每个滑动窗口的位置都会预测多个候选区域，假设每个位置最多预测k个候选框。这样reg层有4k个输出，编码了k个框的坐标信息；cls层对于k个目标/非目标输出2k个分值（cls层为2分类的softmax层。另外也可以拟合k个分值）。k个候选框通过k个参考框进行参数化，称作anchors。将滑动窗固定到一个位置，通过不同的尺度及宽高比，得到不同的anchor，如图2左侧。默认使用3个尺度和3个宽高比，每个滑动位置可得9个anchor。对于W\*H的特征图，总共有WHk个anchor。

**anchor具有平移不变性**。可以在任何位置预测相同的目标。

anchor参数量少。对于vgg16（特征维度512），输出层参数量为
$$512*\left( 4+2 \right)*9=2.8*{ {10}^{4}}$$

**用于回归参考的多尺度anchors**：该文的anchor是解决多尺度和多宽高比问题的新方案。如图3所示，对于多尺度预测有2中常见方案。第一种为图像/特征金字塔方案，如下图a所示。在多尺度上对图像进行缩放，每个尺度的图像提取特征。这种方法有效，但耗时。第二种为在特征图上的多尺度（多宽高比）滑动窗方案，如下图b所示。通常使用不同的滤波器尺寸，如5\*7，7\*5.这种方法可以称作滤波器金字塔。第二种方法通常和第一种方法联合使用。

本文则是建立在anchor金字塔的基础上，更高效。本文的方法参考多尺度和纵横比的anchors对边界框进行分类和回归。只依赖单一尺度上的图像和特征图，并且只使用一个尺寸的滤波器。这种方法可以解决多尺度和多宽高比问题。
 
![3](/assets/post/2021-09-20-fasterRCNN/3scales.png)
_图3_

### P2.2 损失函数

训练RPN网络时，每个anchor是二分类（目标/非目标）。当①anchor和GT框IOU最大，②当anchor和任一GT框的IoU大于0.7时，该anchor即为正样本（目标）。需要注意的是，每个GT框的label可能分配个多个anchor。通常第二个条件就能确定正样本，但为了避免在极端情况下第二个条件找不到正样本，该文还是采用了第一个条件。和所有GT的IoU均小于0.3的anchor为负样本。既不是正样本也不是负样本的anchor不参与训练。

Faster R-CNN使用多任务损失：

$$L\left( \left\{ { {p}_{i}} \right\},\left\{ { {t}_{i}} \right\} \right)=\frac{1}{ { {N}_{cls}}}\sum\limits_{i}{ { {L}_{cls}}\left( { {p}_{i}},p_{i}^{*} \right)}+\lambda \frac{1}{ { {N}_{reg}}}\sum\limits_{i}{p_{i}^{*}{ {L}_{reg}}\left( { {t}_{i}},t_{i}^{*} \right)}$$

其中i为当前batch中anchor的索引，
$${ {p}_{i}}$$
为当前anchor预测为目标的概率。如果anchor为正样本，GT值
$$p_{i}^{*}$$
为1，否则其为0。
$${ {t}_{i}}\in { {R}^{4}}$$
为预测框的4个坐标偏移，
$$t_{i}^{*}$$
为分配给正anchor的GT坐标偏移。
$${ {L}_{cls}}$$
为二分类（目标/非目标）的log损失。回归损失
$${ {L}_{reg}}\left( { {t}_{i}},t_{i}^{*} \right)=R\left( { {t}_{i}}-t_{i}^{*} \right)$$
，其中R为smooth L1损失。
$$p_{i}^{*}{ {L}_{reg}}$$
是指只有
$$p_{i}^{*}\text{=}1$$
的正样本计算回归损失，
$$p_{i}^{*}\text{=0}$$
的负样本不计算回归损失。分类损失cls和回归损失reg层的输出包含
$$\left\{ { {p}_{i}} \right\}$$
和
$$\left\{ { {t}_{i}} \right\}$$
。

cls和reg这两项分别通过
$${ {N}_{cls}}=256$$
和
$${ {N}_{reg}}$$
（anchor位置的数量，近似为2400）进行归一化，并使用权重
$$\lambda =10$$
加权（此时cls和reg权重基本相同）。实验结果表明性能对
$$\lambda $$
不敏感，且上述归一化项并非必须的。

对于边界框回归，使用以下4个参数化的坐标：

$${ {t}_{x}}=\left( x-{ {x}_{a}} \right)/{ {w}_{a}},\quad\quad { {t}_{y}}=\left( y-{ {y}_{a}} \right)/{ {h}_{a}}$$

$${ {t}_{w}}=\log \left( w/{ {w}_{a}} \right),\quad\quad { {t}_{h}}=\log \left( h/{ {h}_{a}} \right)$$

$$t_{x}^{*}=\left( { {x}^{*}}-{ {x}_{a}} \right)/{ {w}_{a}},\quad\quad t_{y}^{*}=\left( { {y}^{*}}-{ {y}_{a}} \right)/{ {h}_{a}}$$

$$t_{w}^{*}=\log \left( { {w}^{*}}/{ {w}_{a}} \right),\quad\quad t_{h}^{*}=\log \left( { {h}^{*}}/{ {h}_{a}} \right)$$

其中x, y, w, h为框的中心坐标和宽高。x, 
$${ {x}_{a}}$$
, 
$${ {x}^{*}}$$
分别为预测框，anchor框，GT框（y, w, h同理）。可以认为是从anchor框到其附近GT框的边界框回归。

该文中在特征地图上用于回归的特征的空间大小相同（3\*3）。考虑到不同的尺寸，学习k个边界框回归器（bounding-box regressors），每个回归器代表一个尺度、一个宽高比。这k个回归器不共享权重。因而即便特征的尺寸/比例固定，由于anchor的设计，仍然可以预测各种尺寸的目标。

### P2.3 训练RPNs

RPN使用bp和sgd训练。每次输入一张图像，得到anchors后，随机选择256个anchors，其中正负样本比例为1:1，如果正样本少于128，则添加负样本。该文没有使用所有的anchors计算损失，因为负样本占多数，会导致训练偏向于负样本。

所有新增加的层的权重通过零均值，标准差为0.01的高斯分布初始化。其它层（如共享的卷积层）通过ImageNet上预训练的模型初始化。该文训练ZF网络的所有层，为了节省内存，训练VGG网络conv3_1及以上层。在PASCAL VOC库训练时，前60k次mini-batches的lr=0.001，剩下20k次mini-batches的lr=0.0001。momentum=0.9，weight decay为0.0005。该文使用caffe训练。

## P3. RPN和fast R-CNN的共享特征

上面介绍了RPN网络的训练部分，下面我们将介绍由使用共享卷积层的RPN和Fast R-CNN（检测网络）的组成的统一网络，如图1所示。

RPN和Fast R-CNN单独训练时，会通过不同的方式更新卷积层。因此，我们需要开发能训练共享卷积层的技术。此处讨论三种训练含有共享特征的网络训练方法：

① 交替训练。先训练RPN，然后使用候选框训练Faster R-CNN。之后再使用Fast R-CNN得到网络初始化RPN，并迭代该过程。本文使用这种方法。

② 近似联合训练。RPN和Fast R-CNN网络在训练阶段合并成一个网络。在每个SGD迭代中，训练快速R-CNN检测器时，前向计算得到的候选区域被视为固定的、预先计算的。反向传播正常计算，但是对于共享层，RPN的损失和Fast R-CNN的损失叠加起来。此方法方案易于实现。但其忽略了候选框坐标的导数也是网络的响应，因此是近似方法。该文实验发现，这种方法得到的结果与交替训练非常接近，但训练时间减少了约25-50%。这个方法包含在作者发布的Python代码中。

③ 非近似联合训练。由于RPN预测的边界框也是输入的函数，另一方面，Fast R-CNN中的RoI池化层将卷积特征和预测的边界框作为输入，因此bp时还应包括损失关于边界框坐标的梯度。上述近似联合训练中忽略了这些梯度。在非近似联合训练的方法中，需要关于边界框坐标可导的RoI池化层。这是一个复杂的问题，可以通过<https://arxiv.org/abs/1512.04412>中的“RoI warping”层解决。

**四步交替训练**。本文采用四步训练算法，通过交替优化学习共享特征：① 通过2.3中的方法训练RPN网络。该网络使用ImageNet预训练模型初始化，并对RPN网络进行微调。② 使用上一步学习到的RPN得到的候选框，通过Fast R-CNN训练单独的检测网络。该监测网络依旧使用ImageNet预训练模型初始化。这时两个网络不共享卷积层。③ 使用检测网络初始化RPN训练，但固定共享卷积层的参数，只微调RPN中独有的层。此时两个网络已经共享卷积层了。④ 固定共享卷积层的参数，微调Fast R-CNN中独有的层。至此，两个网络共享相同的卷积层，并且形成了一个统一的网络。

## P4. 实现细节
我们在单尺度图像上训练和RPN和检测网络。缩放图像，保证宽高中较短的边s=600像素。使用图像金字塔进行多尺度特征提取可能提高准确度，但无法保证速度-准确度的平衡。相比输入图像，ZF和vgg网络最后一层卷积层的stride=16。

该文使用3个尺度，3个宽高比的anchor。3个尺度框的面积分别为128\*128，256\*256，512\*512。3个宽高比分别为1:1，1:2，2:1。这些参数并非为特定数据库仔细选择的。图2（右）显示了该算法在各种尺度和纵横比下的检测能力。表1列出了使用ZF网络学习到的每个anchor的平均大小。需要注意的是，该算法允许预测比潜在的感受野更大的目标。这样的预测并非不可能：如果只有物体的中部可见，仍可以粗略地推断出物体的范围。


![4](/assets/post/2021-09-20-fasterRCNN/4proposalsize.png)
_表1_

需要小心处理跨越图像边界的anchor。训练阶段忽略所有越界的anchor，这些anchor对损失无贡献。对于1000\*600的图像，大约20000（60\*40\*9）个anchor。忽略越界的anchor后，训练阶段一张图像大约有6000个anchor。如果在训练时不忽略越界框，会引入较大且难以纠正的误差，进而训练无法收敛。测试阶段仍旧将全卷机RPN应用到整张图像上。这也会产生越界框，将其剪裁到图像边界即可。

由于RPN的候选框重叠比较高，为了降低冗余，对根据cls分数对候选框使用NMS抑制。NMS的IoU阈值设定为0.7，这样一张图像有大约2000个候选框。NMS不会影响检测准确率，但大大减少了候选框数量。NMS之后，将排名前N的候选框用于检测。
